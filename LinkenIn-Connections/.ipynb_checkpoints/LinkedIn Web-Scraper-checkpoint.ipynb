{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6fc7ac",
   "metadata": {},
   "source": [
    "### LinkedIn Web Scraper\n",
    "\n",
    "This project scraps the LinkedIn site looking at connections of my account to store basic information including name, current role, past work experience, etc. The processed information is stored in a dataframe which could be exported as csv files for further data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "1771dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules for web scraping\n",
    "import requests, time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c667ade",
   "metadata": {},
   "source": [
    "Selenium is used for navigating the Chrome web driver. Beautiful soup is then used to extract the information for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf05e1",
   "metadata": {},
   "source": [
    "#### Initialise Chrome webdriver and login to personal account\n",
    "\n",
    "Input personal username and password in the relevant fields for login purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8e06df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access webdriver application\n",
    "PATH = \"C:/Program Files (x86)/chromedriver.exe\"\n",
    "browser = webdriver.Chrome(PATH)\n",
    "\n",
    "# Get to login page\n",
    "browser.get(\"https://www.linkedin.com/uas/login\")\n",
    "\n",
    "# Input username and password\n",
    "username=\"\"\n",
    "password=\"\"\n",
    "\n",
    "# Find html elements for username and password\n",
    "usernameID = browser.find_element_by_id('username')\n",
    "usernameID.send_keys(username)\n",
    "passwordID = browser.find_element_by_id('password')\n",
    "passwordID.send_keys(password)\n",
    "\n",
    "# Submit username and password for login\n",
    "passwordID.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70c4abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure that webdriver is loaded till the bottom of the page \n",
    "# before beautiful soup object is initialised to parse info\n",
    "def scrollToBottom():\n",
    "    # Pause time\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for i in range(3):\n",
    "        # Sroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f23d17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an instance of beautiful soup based on current browser page\n",
    "def buildSoup():\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "da3e5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information while on a profile page\n",
    "def getInfo(soup):\n",
    "    \n",
    "    # Get name of person, title of their role\n",
    "    infoSection = soup.find(\"div\", {\"class\": \"ph5\"})\n",
    "    leftDiv = infoSection.find(\"div\", {\"class\": \"pv-text-details__left-panel\"})\n",
    "    name = leftDiv.find_all(\"div\")[0].find(\"h1\").text.strip()\n",
    "    title = leftDiv.find_all(\"div\")[1].text.strip()\n",
    "    \n",
    "    # Get organisation the person works in, location of their role\n",
    "    rightDiv = infoSection.find(\"ul\", {\"class\": \"pv-text-details__right-panel\"})\n",
    "    organisation = rightDiv.find(\"h2\").text.strip()\n",
    "    location = rightDiv.next_sibling.next_sibling.find(\"span\").text.strip()\n",
    "    \n",
    "    # Get work experience of the person\n",
    "    exp = soup.find(lambda tag: tag.name == \"span\" and \"Experience\" in tag.text)\n",
    "    if exp == None:\n",
    "        return [name, title, organisation, location]\n",
    "    expSection = exp.find_parent(\"section\")\n",
    "    expSection = expSection.find(\"ul\")\n",
    "    jobList = expSection.find_all(\"li\")\n",
    "    # Initialise a list to store informatino about their part jobs\n",
    "    jobs = []\n",
    "    for job in jobList:\n",
    "        jobTitle = job.find(\"span\", {\"class\": \"t-bold mr1 hoverable-link-text\"})\n",
    "        jobCompany = job.find(\"span\", {\"class\": \"t-14 t-normal\"})\n",
    "        jobPeriod = job.find(\"span\", {\"class\": \"t-14 t-normal t-black--light\"})\n",
    "        if jobTitle is not None:\n",
    "            jobTitle = jobTitle.find(\"span\").text\n",
    "            jobCompany = jobCompany.find(\"span\").text\n",
    "            jobCompany = jobCompany.split(\"路\")[0]\n",
    "            jobPeriod = jobPeriod.find(\"span\").text\n",
    "            jobDates = jobPeriod.split(\"-\")\n",
    "            if len(jobDates) == 2:\n",
    "                jobStart = jobDates[0].strip()\n",
    "                jobEnd = jobDates[1].split(\"路\")[0].strip()\n",
    "            else:\n",
    "                jobStart = jobPeriod.split(\"路\")[0].strip()\n",
    "                jobEnd = jobPeriod.split(\"路\")[0].strip()\n",
    "\n",
    "            jobs.extend([jobTitle, jobCompany, jobStart, jobEnd])\n",
    "    \n",
    "    return [name, title, organisation, location] + jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "17007ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store extracted information into a dataframe\n",
    "def storeDataFrame(infoDF, infoList):\n",
    "    name = infoList[0]\n",
    "    title = infoList[1]\n",
    "    organisation = infoList[2]\n",
    "    location = infoList[3]\n",
    "    \n",
    "    # Initialise dictionary to store basic information\n",
    "    infoDict = {'Name': name, 'Title': title, 'Organisation': organisation, 'Location': location}\n",
    "    \n",
    "    # Add each job into the dictionary\n",
    "    count = 1\n",
    "    for idx in range(4, len(infoList)):\n",
    "        if idx % 4 == 0:\n",
    "            key = \"Job Title \" + str(count)\n",
    "        elif idx % 4 == 1:\n",
    "            key = \"Comany \" + str(count)\n",
    "        elif idx % 4 == 2:\n",
    "            key = \"Start Date \" + str(count)\n",
    "        elif idx % 4 == 3:\n",
    "            key = \"End Date \" + str(count)\n",
    "            count += 1\n",
    "        infoDict[key] = infoList[idx]\n",
    "        \n",
    "    # Add extra row for each person into dictionary \n",
    "    new_row = pd.DataFrame(infoDict, index = [0])\n",
    "    infoDF = pd.concat([new_row, infoDF]).reset_index(drop = True)\n",
    "    return infoDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "86cfb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create link to connections list page\n",
    "def createLink(connectionHref):\n",
    "    connectionLink = \"https://www.linkedin.com/\" + connectionHref\n",
    "    return connectionLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5840d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add connection urls to a queue\n",
    "def addProfile(profileQueue, connectionLink):\n",
    "    \n",
    "    # Access the connectionLink from initial link\n",
    "    browser.get(connectionLink)\n",
    "    \n",
    "    # while loop that runs as long as there are extra pages of connections to access\n",
    "    while True:\n",
    "        scrollToBottom()\n",
    "        connectionSoup = buildSoup()\n",
    "        \n",
    "        connectionList = connectionSoup.find_all(\"span\", {\"class\": \"entity-result__title-text t-16\"})\n",
    "\n",
    "        for connection in connectionList:\n",
    "            connectionTag = connection.contents[1]\n",
    "            profileHref = connectionTag[\"href\"]\n",
    "            if profileHref not in profileQueue:\n",
    "                profileQueue.append(profileHref)\n",
    "        \n",
    "        nextButton = WebDriverWait(browser, 50).until(EC.element_to_be_clickable((By.CLASS_NAME, \"artdeco-pagination__button--next\")))\n",
    "        if 'artdeco-button--disabled' in nextButton.get_attribute('class'):\n",
    "            break;\n",
    "        nextButton.click()\n",
    "            \n",
    "    return profileQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f016f2",
   "metadata": {},
   "source": [
    "Functions above used to extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a dataframe to store information\n",
    "infoDF = pd.DataFrame()\n",
    "\n",
    "# Initialise a queue for processing profiles\n",
    "profileQueue = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35386711",
   "metadata": {},
   "source": [
    "Input relevant starting page to access connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "e89f01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access link to personal page\n",
    "initialLink = \"\"\n",
    "\n",
    "# Access the initial link using webdriver\n",
    "browser.get(initialLink)\n",
    "\n",
    "# Scroll to bottom of page to access entire dom\n",
    "scrollToBottom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4eae1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a beautiful soup instance for current profile page\n",
    "soup = buildSoup()\n",
    "\n",
    "# Extract information from soup\n",
    "infoList = getInfo(soup)\n",
    "\n",
    "# Store information into data frame\n",
    "infoDF = storeDataFrame(infoDF, infoList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536cb2d",
   "metadata": {},
   "source": [
    "#### Access connections to get information\n",
    "\n",
    "Use selenium functions to look through all connections and storing them into a list for individual profile extraction later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47c549ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get link to list of connections\n",
    "connectionSpan = soup.find(\"span\", {\"class\": \"link-without-visited-state\"})\n",
    "connectionTag = connectionSpan.parent\n",
    "connectionHref = connectionTag[\"href\"]\n",
    "connectinLink = createLink(connectionHref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all connection urls\n",
    "profileQueue = addProfile(profileQueue, connectionLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0521d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of connecitions: 254\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of connecitions: {len(profileQueue)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d931b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access each profile to extract information\n",
    "for idx, profileUrl in enumerate(profileQueue):\n",
    "    browser.get(profileUrl)\n",
    "    scrollToBottom()\n",
    "    \n",
    "    # Create a beautiful soup instance for current profile page\n",
    "    soup = buildSoup()\n",
    "\n",
    "    # Extract information from soup\n",
    "    infoList = getInfo(soup)\n",
    "    print(f\"Processing {infoList[0]}'s profile\")\n",
    "\n",
    "    # Store information into data frame\n",
    "    infoDF = storeDataFrame(infoDF, infoList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba3b64",
   "metadata": {},
   "source": [
    "Export data into csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "aeb447ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "infoDF.to_csv('linkedin-info.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
